---
title: Converting Folder of GRIB files to CSV in time series
format: html
author: Jack Bienvenue
Date: "February 3, 2025"
---

We discovered how to properly bring in the GRIB files after some difficulty in our initial data cleaning script. Now, let's write some code to allow for the combination of all the GRIB files in our folder into a single CSV for us to be able to process it more effectively.

```{python}
#| echo: false
#| eval: true

# Package import
import pandas as pd
import cfgrib
import os
```

```{python}
#| echo: true
#| eval: true

"""
Here, I'm electing to write this process as a function so that in the future, you or I could reuse this function or adapt it to create similar long-term time series data for weather data from ERA5. 

ARGUMENTS:
earliest_file - the EARLIEST chronological file (NAME only, no prefixes)
latest_file - the LATEST chronological file (NAME only, no prefixes)
input_directory - input directory which hosts the files (use relative path)
output_directory_path - output path for the NEW directory that is meant to store the new, grid-cell-specific CSVs
hourly - whether or not to export data at an hourly resolution

NOTES:
Dependencies: pandas, cfgrib, os
"""

def grib_folder_processing(earliest_file, latest_file, input_directory, output_directory_path, hourly=False):

    ## FUNCTION PHASE 1: TAKING GRIB FILES FROM FOLDER, CREATING ONE LARGE DF WITH ALL GRID CELL CENTROIDS

    # Extract year and month from the earliest and latest file names
    earliest_year = int(earliest_file.split('_')[-2])  # Extract year from filename (second-to-last part)
    earliest_month = int(earliest_file.split('_')[-1].split('.')[0])  # Extract month from filename (last part before extension)
    latest_year = int(latest_file.split('_')[-2])  # Extract year from filename (second-to-last part)
    latest_month = int(latest_file.split('_')[-1].split('.')[0])  # Extract month from filename (last part before extension)

    # Initialize an empty list to store DataFrames
    df_list = []

    # Loop through years and months
    current_year, current_month = earliest_year, earliest_month
    while (current_year < latest_year) or (current_year == latest_year and current_month <= latest_month):

        # Construct the file name for the current year and month
        file_name = f"download_ERA5_LAND_package_{current_year}_{current_month:02d}.grib"
        file_path = os.path.join(input_directory, file_name)

        # Debug: Print the file path being checked
        #####print(f"Checking file: {file_path}")

        # Check if the file exists and is a GRIB file (not an index file)
        if os.path.exists(file_path) and file_name.endswith(".grib"):

            if not hourly:
                try:
                    # Debug: Print a message before attempting to read the file
                    #####print(f"Attempting to read file: {file_name}")

                    # Read the GRIB file and convert to DataFrame
                    data = cfgrib.open_dataset(
                        file_path,
                        backend_kwargs={
                            'filter_by_keys': {
                                'typeOfLevel': 'surface',
                                'step': 1  # 1 hour step size
                            }
                        }
                    ) # Attempt to resolve issue with steps

                    
                    df = data.to_dataframe()
                    df_list.append(df)
                    #####print(f"Successfully processed file: {file_name}")

                except Exception as e:
                    print(f"Error processing file {file_name}: {e}")

            elif hourly:
                for step in range(1, 25):

                    # Read the GRIB file and convert to DataFrame
                    hourly_data = cfgrib.open_dataset(
                            file_path,
                            backend_kwargs={
                                'filter_by_keys': {
                                    'typeOfLevel': 'surface',
                                    'step': step  # variable time of day
                                }
                            }
                    ) # Attempt to resolve issue with steps
                    hourly_df = hourly_data.to_dataframe()

                    df_list.append(hourly_df)

        else:
            print(f"File {file_name} not found in directory or is not a valid GRIB file.")

        # Move to the next month
        current_month += 1
        if current_month > 12:  # Reset month and increment year if necessary
            current_month = 1
            current_year += 1

    # Check if any files were processed
    if len(df_list) == 0:
        raise ValueError("No valid GRIB files were processed. Check the input directory and file names.")

    # Since latitude and longitude are being used as a part of a multi-index with time, let's replicate the latitude and longitude columns so that they are still accessible after the concatenation

    # Solving the multi-index issue to assign new columns:
    for i, df in enumerate(df_list):

        # Reset index to move latitude and longitude to columns
        df = df.reset_index()

        # Create latitude1 and longitude1 columns
        df['latitude1'] = df['latitude'].round(5)
        df['longitude1'] = df['longitude'].round(5)

        # Set index back to MultiIndex with time, latitude, and longitude
        df.set_index(['time', 'latitude', 'longitude'], inplace=True)

        # Reassign the modified DataFrame back to the list
        df_list[i] = df

    # Combine all DataFrames into one
    combined_df = pd.concat(df_list, ignore_index=True)  

    print(combined_df.columns)

    # Sort the DataFrame chronologically
    combined_df.sort_index(inplace=True)

    #-----------------------------------------------------------------------------------

    ## FUNCTION PHASE 2: COLLAPSING NEW LARGE DF INTO MANY INIDIVUDAL GRID CELL CSVs

    '''
    In this section, we have a few steps:

    1. Isolate individual points throughout timeseries
    2. Sort them into their unique dataframes
    3. Go into each dataframe and make sure the information is sorted chronologically
    4. Create new directory to store the new CSVs
    5. Export the CSVs into the new directory with appropriate names

    '''

    #1. Isolate individual points throughout timeseries

        #1.a First, to save on storage space, let's drop some unnecessary columns. After inspection of the df,
        # it looks as though columns "surface,", "step", and "number" aren't actually providing us with useful information. We'll get rid of those here:
        # furthermore, for the purposes of our analysis, we 

    df = combined_df.drop(columns=['number', 'step', 'surface'])
    df = df.drop(columns=['d2m', 'sp', 'lai_hv', 'lai_lv', 'sf']) # Drop extraneous columns

    #2. Sort individual points into their unique dataframes

        # Here, we'll use a group by clause to yield a list of dataframes, each corresponding to an individual point. This particular method preserves the lat/long combo as both the index for the new df and as accessible columns in the resulting dfs:

    grouped = [group.set_index(['latitude1', 'longitude1']).reset_index() for _, group in df.groupby(['latitude1', 'longitude1'])]


    #3. Go into each dataframe and make sure the information is sorted chronologically

    for i, group_df in enumerate(grouped):

    #3.a Convert 'valid_time' column to datetime
        group_df['valid_time'] = pd.to_datetime(group_df['valid_time'], format='%m/%d/%Y %I:%M')
    
    #3.b Sort each df by 'time'
        group_df = group_df.sort_values(by='valid_time')

    #4. Create new directory to store the new CSVs

    #4.a Create nested function to make directory (if necessary, which in most cases should be) for CSV storage

    def create_directory(output_directory_path):
        if not os.path.exists(output_directory_path):
            os.makedirs(output_directory_path)

    #5. Export the CSVs into the new directory with appropriate names

        #5.a. Create function to name files appropriately as they are being exported 

    def export_dfs_to_csv(dfs, output_directory_path): #dfs is a list!
        create_directory(output_directory_path)  # Ensure directory exists

            # Iterate through the list of DataFrames
        for idx, group_df in enumerate(dfs):
            # Define a naming convention based on latitude and longitude
            # For example: "latitude_longitude_df_1.csv"
            lat = group_df['latitude1'].iloc[0]  # Get the first value (assuming all values are the same for each group)
            lon = group_df['longitude1'].iloc[0]  # Same as above for longitude

            lon = lon.round(5) # for consistency and brevity

            # Rename columns:
            group_df = group_df.rename(columns={'latitude1': 'latitude', 'longitude1': 'longitude', 'valid_time': 'time'})
            
            # Create a filename using latitude, longitude, and an index
            filename = f"lat_{lat}_lon_{lon}_time_series_weather.csv"

            # Replace dots in latitude and longitude with underscores
            filename = filename.rsplit('.', 1)  # Split only on the last dot (the file extension)
            filename[0] = filename[0].replace('.', '_')  # Replace dots in the main part of the filename

            # Join the filename back together
            filename = '.'.join(filename)
            
            # Construct the full path for the CSV file
            full_path = os.path.join(output_directory_path, filename)

            # Compensate for error:

            if hourly:
                group_df = group_df.sort_values(by='time')

            # Export the DataFrame to CSV
            group_df.to_csv(full_path, index=False)

        #5.b. Wrap it up by using the functions to export!

    export_dfs_to_csv(grouped, output_directory_path)

```

Below, you can execute the function by using your parameters of choice

```{python}
#| echo: true
#| eval: true

'''
ARGUMENTS:
earliest_file - the EARLIEST chronological file (NAME only, no prefixes)
latest_file - the LATEST chronological file (NAME only, no prefixes)
input_directory - input directory which hosts the files (use relative path)
output_directory_path - output path for the NEW directory that is meant to store the new, grid-cell-specific CSVs

NOTE:

RUNNING HOURLY=TRUE FOR SPANS OF TIME BEYOND 5 YEAR INTERVALS MAY CAUSE THE KERNEL TO DIE WHILE PROCESSING. FOR BEST RESULTS, BATCH THE OPERATIONS AND RESOLVE BATCHING IN ./merging_batched_csvs.qmd
'''

# Function use:
grib_folder_processing("download_ERA5_LAND_package_2014_01.grib", "download_ERA5_LAND_package_2017_12.grib", "/Volumes/JB_Fortress_L3/EEC/data_CT", "/Volumes/JB_Fortress_L3/EEC/hourly_csvs_batch8", hourly = True)

```

NOTE: THIS FILE EXPORTS GRIB FILES AT A DAILY SCALE. THE ORIGINAL GRIB FILES SUPPORT AN HOURLY RESOLUTION, AND NOTE THE FOLLOWING STRUCTURAL COMPONENTS:
1. The Grib files begin with an empty day (24 hours) which corresponds to the final day of the previous month
2. The Grib files end with an empty hour (midnight of the first day of the next month)