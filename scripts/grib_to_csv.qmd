---
title: Converting Folder of GRIB files to CSV in time series
format: html
author: Jack Bienvenue
Date: "February 3, 2025"
---

We discovered how to properly bring in the GRIB files after some difficulty in our initial data cleaning script. Now, let's write some code to allow for the combination of all the GRIB files in our folder into a single CSV for us to be able to process it more effectively.

```{python}
#| echo: false
#| eval: true

# Package import
import pandas as pd
import cfgrib
import os
```

```{python}
#| echo: true
#| eval: true

"""
Here, I'm electing to write this process as a function so that in the future, you or I could reuse this function or adapt it to create similar long-term time series data for weather data from ERA5. 

ARGUMENTS:
earliest_file - the EARLIEST chronological file (NAME only, no prefixes)
latest_file - the LATEST chronological file (NAME only, no prefixes)
input_directory - input directory which hosts the files (use relative path)
large_csv_output_path - output path for new csv (name it at the end of the path)
output_directory_path - output path for the NEW directory that is meant to store the new, grid-cell-specific CSVs

NOTES:
Dependencies: pandas, cfgrib, os
"""

def grib_folder_processing(earliest_file, latest_file, input_directory, large_csv_output_path, output_directory_path):

    ## FUNCTION PHASE 1: TAKING GRIB FILES FROM FOLDER, CREATING ONE LARGE CSV WITH ALL GRID CELL CENTROIDS

    # Extract year and month from the earliest and latest file names
    earliest_year = int(earliest_file.split('_')[-2])  # Extract year from filename (second-to-last part)
    earliest_month = int(earliest_file.split('_')[-1].split('.')[0])  # Extract month from filename (last part before extension)
    latest_year = int(latest_file.split('_')[-2])  # Extract year from filename (second-to-last part)
    latest_month = int(latest_file.split('_')[-1].split('.')[0])  # Extract month from filename (last part before extension)

    # Initialize an empty list to store DataFrames
    df_list = []

    # Loop through years and months
    current_year, current_month = earliest_year, earliest_month
    while (current_year < latest_year) or (current_year == latest_year and current_month <= latest_month):
        # Construct the file name for the current year and month
        file_name = f"download_ERA5_LAND_package_{current_year}_{current_month:02d}.grib"
        file_path = os.path.join(input_directory, file_name)

        # Debug: Print the file path being checked
        print(f"Checking file: {file_path}")

        # Check if the file exists and is a GRIB file (not an index file)
        if os.path.exists(file_path) and file_name.endswith(".grib"):
            try:
                # Debug: Print a message before attempting to read the file
                print(f"Attempting to read file: {file_name}")

                # Read the GRIB file and convert to DataFrame
                hourly_data = cfgrib.open_dataset(file_path, backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'step': 1}})
                hourly_df = hourly_data.to_dataframe()
                df_list.append(hourly_df)
                print(f"Successfully processed file: {file_name}")
            except Exception as e:
                print(f"Error processing file {file_name}: {e}")
        else:
            print(f"File {file_name} not found in directory or is not a valid GRIB file.")

        # Move to the next month
        current_month += 1
        if current_month > 12:  # Reset month and increment year if necessary
            current_month = 1
            current_year += 1

    # Check if any files were processed
    if len(df_list) == 0:
        raise ValueError("No valid GRIB files were processed. Check the input directory and file names.")

    # Combine all DataFrames into one
    combined_df = pd.concat(df_list)

    # Sort the DataFrame chronologically
    combined_df.sort_index(inplace=True)

    # Save the combined DataFrame to a CSV file
    #####combined_df.to_csv(large_csv_output_path)                      # For now, this is commented out because we are now attempting to export just the little CSVs
    #####print(f"Combined data saved to {large_csv_output_path}")

    #-----------------------------------------------------------------------------------

    ## FUNCTION PHASE 2: COLLAPSING NEW LARGE DF INTO MANY INIDIVUDAL GRID CELL CSVs

    '''
    In this section, we have a few steps:

    1. Isolate individual points throughout timeseries
    2. Sort them into their unique dataframes
    3. Go into each dataframe and make sure the information is sorted chronologically
    4. Create new directory to store the new CSVs
    5. Export the CSVs into the new directory with appropriate names

    '''

    #1. Isolate individual points throughout timeseries

        #1.a First, to save on storage space, let's drop some unnecessary columns. After inspection of the df,
        # it looks as though columns "surface," "valid_time," "step", and "number" aren't actually providing us with useful information. We'll get rid of those here:

    combined_df = combined_df.drop(columns=['number', 'step', 'surface', 'valid_time'])

    #2. Sort individual points into their unique dataframes

        # Here, we'll use a group by clause to yield a list of dataframes, each corresponding to an individual point:

    grouped = [group for _, group in df.groupby(['latitude', 'longitude'])]

    #3. Go into each dataframe and make sure the information is sorted chronologically

    

    #4. Create new directory to store the new CSVs

    #5. Export the CSVs into the new directory with appropriate names

        #5.a. Form new directory using name given by the argument "output_directory_path"

        #5.b. Name files appropriately as they are being exported 

# Example usage:
# grib_folder_processing("download_ERA5_LAND_package_1979_01.grib", "download_ERA5_LAND_package_1980_11.grib", "../data/sample_data/", "../data/sample_output1.csv")
```

Let's try to execute this function:

```{python}
#| echo: false
#| eval: true

grib_folder_processing("download_ERA5_LAND_package_1979_01.grib", "download_ERA5_LAND_package_1980_11.grib", "../data/sample_data/", "../data/output1.csv")

```