---
title: Converting Folder of GRIB files to CSV in time series
format: html
author: Jack Bienvenue
Date: "February 3, 2025"
---

We discovered how to properly bring in the GRIB files after some difficulty in our initial data cleaning script. Now, let's write some code to allow for the combination of all the GRIB files in our folder into a single CSV for us to be able to process it more effectively.

```{python}
#| echo: false
#| eval: true

# Package import
import pandas as pd
import cfgrib
import os
```

```{python}
#| echo: true
#| eval: true

"""
Here, I'm electing to write this process as a function so that in the future, you or I could reuse this function or adapt it to create similar long-term time series data for weather data from ERA5. 

ARGUMENTS:
earliest_file - the EARLIEST chronological file (NAME only, no prefixes)
latest_file - the LATEST chronological file (NAME only, no prefixes)
input_directory - input directory which hosts the files (use relative path)
large_csv_output_path - output path for new csv (name it at the end of the path)
output_directory_path - output path for the NEW directory that is meant to store the new, grid-cell-specific CSVs

NOTES:
Dependencies: pandas, cfgrib, os
"""

def grib_folder_processing(earliest_file, latest_file, input_directory, large_csv_output_path, output_directory_path):

    ## FUNCTION PHASE 1: TAKING GRIB FILES FROM FOLDER, CREATING ONE LARGE CSV WITH ALL GRID CELL CENTROIDS

    # Extract year and month from the earliest and latest file names
    earliest_year = int(earliest_file.split('_')[-2])  # Extract year from filename (second-to-last part)
    earliest_month = int(earliest_file.split('_')[-1].split('.')[0])  # Extract month from filename (last part before extension)
    latest_year = int(latest_file.split('_')[-2])  # Extract year from filename (second-to-last part)
    latest_month = int(latest_file.split('_')[-1].split('.')[0])  # Extract month from filename (last part before extension)

    # Initialize an empty list to store DataFrames
    df_list = []

    # Loop through years and months
    current_year, current_month = earliest_year, earliest_month
    while (current_year < latest_year) or (current_year == latest_year and current_month <= latest_month):
        # Construct the file name for the current year and month
        file_name = f"download_ERA5_LAND_package_{current_year}_{current_month:02d}.grib"
        file_path = os.path.join(input_directory, file_name)

        # Debug: Print the file path being checked
        #####print(f"Checking file: {file_path}")

        # Check if the file exists and is a GRIB file (not an index file)
        if os.path.exists(file_path) and file_name.endswith(".grib"):
            try:
                # Debug: Print a message before attempting to read the file
                #####print(f"Attempting to read file: {file_name}")

                # Read the GRIB file and convert to DataFrame
                hourly_data = cfgrib.open_dataset(file_path, backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'step': 1}})
                hourly_df = hourly_data.to_dataframe()
                df_list.append(hourly_df)
                #####print(f"Successfully processed file: {file_name}")
            except Exception as e:
                print(f"Error processing file {file_name}: {e}")
        else:
            print(f"File {file_name} not found in directory or is not a valid GRIB file.")

        # Move to the next month
        current_month += 1
        if current_month > 12:  # Reset month and increment year if necessary
            current_month = 1
            current_year += 1

    # Check if any files were processed
    if len(df_list) == 0:
        raise ValueError("No valid GRIB files were processed. Check the input directory and file names.")

    # Since latitude and longitude are being used as a part of a multi-index with time, let's replicate the latitude and longitude columns so that they are still accessible after the concatenation

    # Solving the multi-index issue to assign new columns:
    for i, df in enumerate(df_list):
        df = df.reset_index()  # Reset index to move latitude and longitude to columns
        df['latitude1'] = df['latitude'].round(3)  # Create latitude1 column
        df['longitude1'] = df['longitude']  # Create longitude1 column
        df.set_index(['time', 'latitude', 'longitude'], inplace=True)  # Set index back to MultiIndex
        
        # Reassign the modified DataFrame back to the list
        df_list[i] = df

    # Combine all DataFrames into one
    combined_df = pd.concat(df_list, ignore_index=True)  ##### PROBLEM

    #####print("Combined_df columns:", combined_df.columns)

    # Sort the DataFrame chronologically
    combined_df.sort_index(inplace=True)

    # Save the combined DataFrame to a CSV file
    #####combined_df.to_csv(large_csv_output_path)                      # For now, this is commented out because we are now attempting to export just the little CSVs
    #####print(f"Combined data saved to {large_csv_output_path}")

    #-----------------------------------------------------------------------------------

    ## FUNCTION PHASE 2: COLLAPSING NEW LARGE DF INTO MANY INIDIVUDAL GRID CELL CSVs

    '''
    In this section, we have a few steps:

    1. Isolate individual points throughout timeseries
    2. Sort them into their unique dataframes
    3. Go into each dataframe and make sure the information is sorted chronologically
    4. Create new directory to store the new CSVs
    5. Export the CSVs into the new directory with appropriate names

    '''

    #1. Isolate individual points throughout timeseries

        #1.a First, to save on storage space, let's drop some unnecessary columns. After inspection of the df,
        # it looks as though columns "surface,", "step", and "number" aren't actually providing us with useful information. We'll get rid of those here:

    df = combined_df.drop(columns=['number', 'step', 'surface'])

    #2. Sort individual points into their unique dataframes

        # Here, we'll use a group by clause to yield a list of dataframes, each corresponding to an individual point. This particular method preserves the lat/long combo as both the index for the new df and as accessible columns in the resulting dfs:

    grouped = [group.set_index(['latitude1', 'longitude1']).reset_index() for _, group in df.groupby(['latitude1', 'longitude1'])]


    #3. Go into each dataframe and make sure the information is sorted chronologically

    for i, group_df in enumerate(grouped):

    #3.a Convert 'valid_time' column to datetime
        group_df['valid_time'] = pd.to_datetime(group_df['valid_time'], format='%m/%d/%Y %I:%M:%S %p')
    
    #3.b Sort each df by 'time'
        group_df = group_df.sort_values(by='valid_time')

    #4. Create new directory to store the new CSVs

    #4.a Create nested function to make directory (if necessary, which in most cases should be) for CSV storage

    def create_directory(output_directory_path):
        if not os.path.exists(output_directory_path):
            os.makedirs(output_directory_path)

    #5. Export the CSVs into the new directory with appropriate names

        #5.a. Create function to name files appropriately as they are being exported 

    def export_dfs_to_csv(dfs, output_directory_path): #dfs is a list!
        create_directory(output_directory_path)  # Ensure directory exists

            # Iterate through the list of DataFrames
        for idx, group_df in enumerate(dfs):
            # Define a naming convention based on latitude and longitude
            # For example: "latitude_longitude_df_1.csv"
            lat = group_df['latitude1'].iloc[0]  # Get the first value (assuming all values are the same for each group)
            lon = group_df['longitude1'].iloc[0]  # Same as above for longitude
            
            # Create a filename using latitude, longitude, and an index
            filename = f"lat_{lat}_lon_{lon}_time_series_weather.csv"

            # Replace dots in latitude and longitude with underscores
            filename = filename.rsplit('.', 1)  # Split only on the last dot (the file extension)
            filename[0] = filename[0].replace('.', '_')  # Replace dots in the main part of the filename

            # Join the filename back together
            filename = '.'.join(filename)

            
            # Construct the full path for the CSV file
            full_path = os.path.join(output_directory_path, filename)

            # Export the DataFrame to CSV
            group_df.to_csv(full_path, index=False)

        #5.b. Wrap it up by using the functions to export!

    export_dfs_to_csv(grouped, output_directory_path)

```

Let's try to execute this function:

```{python}
#| echo: false
#| eval: true

grib_folder_processing("download_ERA5_LAND_package_1979_01.grib", "download_ERA5_LAND_package_1980_11.grib", "../data/sample_data/", "../data/output1.csv", "../data/trial_directory_for_fxn/")

```