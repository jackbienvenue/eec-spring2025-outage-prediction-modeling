---
title: Data Cleaning Sample, GRIB Files
format: html
author: Jack Bienvenue
Date: "January 31, 2025"
---

``` {python}
#| echo: false

# Package Import
import pandas as pd
import numpy as np
import cfgrib

import folium
```

We will begin the data cleaning process by transitioning GRIB files into dataframes so that we can later collect them into a single CSV for further analysis.

GRIB files tend to be very tricky, and in our GRIB files I have found a persistent issue that comes when opening and reading them. This occurs, I presume, because there are two internal "step" (interval) systems being used, a 1-hour step, and a 24-hour step.

Below, I discovered a way to work around the issue by reading in a GRIB file with the backend keyword argument solved for the step lengths.



```{python}
# Set the GRIB file path:

grib_path = "../data/data_CT/download_ERA5_LAND_package_1982_10.grib"

# Read in hourly data by using 1-hour intervals:

hourly_data = cfgrib.open_dataset(grib_path, backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'step': 1}})

# Read in daily data by using 24-hour intervals:

daily_data = cfgrib.open_dataset(grib_path, backend_kwargs={'filter_by_keys': {'typeOfLevel': 'surface', 'step': 24}})

# Now, print these as dataframes:

hourly_df = hourly_data.to_dataframe()
daily_df = daily_data.to_dataframe()

# Let's see more rows to check for NaN's:

pd.set_option('display.max_rows', 100)

# Display dataframes (if you would like to see them)
#####display(hourly_df)
#####display(daily_df)

# Count NaNs in column of new dfs:

nan_count = daily_df['t2m'].isna().sum()

# Print count of NAs in column:

print('Count of NaNs:', nan_count)
```

Across the daily & hourly dfs and over the time series, there seems to always be a nan_count of 1550. Let's investigate this.

```{python}
#| eval: true
#| echo: true

# First, make subset of all NaNs:

all_nan_df = hourly_df[hourly_df['lai_hv'].isna()]

print(all_nan_df)
```

Note: all NaNs in this dataset coexist with other NaNs.

```{python}
#| eval: true
#| echo: true

# Check columns to ensure 'latitude' and 'longitude' are correctly named
print(all_nan_df.columns)

# Reset index if needed
all_nan_df = all_nan_df.reset_index()

# Now filter rows with NaN values in latitude and longitude
df_cleaned = all_nan_df.dropna(subset=['latitude', 'longitude'])

# Create a folium map centered around the mean latitude and longitude
map_center = [df_cleaned['latitude'].mean(), df_cleaned['longitude'].mean()]
my_map = folium.Map(location=map_center, zoom_start=10)

# Plot each point on the map
for idx, row in df_cleaned.iterrows():
    folium.Marker(
        location=[row['latitude'], row['longitude']],
        popup=row['time']
    ).add_to(my_map)

# Display the map (works in Jupyter or saves to file)
my_map

```


### APPENDIX:

#### Error - skipping variables:

Error mentions 'skt', short name for variable "Skin Temperature"---"This parameter is the temperature of the surface of the Earth.

The skin temperature is the theoretical temperature that is required to satisfy the surface energy balance. It represents the temperature of the uppermost surface layer, which has no heat capacity and so can respond instantaneously to changes in surface fluxes. Skin temperature is calculated differently over land and sea.""

Also, it skips 'rsn', 'sd', 'src'