---
author: Jack Bienvenue
title: Windowing for Driver Maximization
format: html
date: 11 March 2025
---

# Introduction 

When constructing the outage prediction model, we make the assumption that outages are caused primarily by high intensity drivers causing disruptions which lead to outages. 

Outage events are sorted into individual days. For storm events, the distinction of days is arbitrary. This file introduces a method for windowing a 24-hour period before and after an outage event is recorded (for a 48 hour window) and identifying the maximum mean daily values for drivers within the 24 fabricated "pseudo-days" which can be formed by incrementing 24 hour periods hourly within the 48 hour window.

``` {python}
# Package Import
import pandas as pd
import geopandas as gpd
import folium
import matplotlib.pyplot as plt
import os
import warnings

# After validating that these messsages do not apply for this script, suppress:
warnings.filterwarnings("ignore", 
    message="Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect.")
```

Begin with the shapefile of municipalities for Connecticut. (Here, it is fabricated using a shapefile of CT and the set of Northeast towns)

# Data Preparation

``` {python}
#| echo: true

# Step 1: Load northeast towns, ct boundary shapefile
towns_gdf = gpd.read_file('../data/northeast_towns_shapefile/Northeast_Town_Polygon.shp')
ct_gdf = gpd.read_file('../data/ct_shapefile/Connecticut_Poly.shp')

# Step 2: Put shapefiles in the same CRS (if they are not already)
towns_gdf = towns_gdf.to_crs(ct_gdf.crs)

# Step 3: Perform the intersection, getting CT towns
ct_towns_gdf = gpd.overlay(towns_gdf, ct_gdf, how='intersection')

# Step 4: save result
ct_towns_gdf.to_file('../data/ct_towns/ct_towns.shp')

# Plot to confirm
ct_towns_gdf.plot()
```

# Function Construction

``` {python}
#| eval: true

# Data Import

def windowing_maxima(town_shapefile = "../data/ct_towns/ct_towns.shp", 
    outage_data = "../data/town_outages.csv", 
    weather_data_dir = "/Volumes/JB_Fortress_L3/EEC/merged_csvs", 
    grid_cell_shapefile = "../data/grid_cell_shapefile/grid_cells.shp"):

    '''
    This function...

    ARGUMENTS:

    town_shapefile - the argument takes the town shapefile as an input 
    as a spatial merging field

    outage_data - the csv file holding information about outages events 
    by town

        Fields for outage_data file:
            - town ("Town")
            - storm_start_datetime ('YYYY-MM-DD HH:MM:SS+00:00')
            - outages (count data)
            - customers_affected (count data)

    weather_data_dir - The directory which is home to HOURLY csv time 
    series of weather data

    grid_cell_shapefile - the relative path to the shapefile containing 
    the grid cells for the state of interest

    '''

#####-----   FUNCTION PHASE 1: READ IN DATA, MERGE GRID CELLS TO TOWNS    -----#####

    # Phase 1.1: READ IN SHAPEFILE FOR CT TOWNS, GRID CELLS

    ct_towns_shp = town_shapefile # from arg
    grid_cells_shp = grid_cell_shapefile # from arg

    # Phase 1.2: READ IN OUTAGE DATA

    gdf = gpd.read_file(outage_data)

    # Phase 1.3: Define windows for events:

        ### FOR SPEED OF DEVELOPMENT:

    gdf = gdf.tail()

        ### -------DELETE LATER---------

    ## Remove extraneous "+00:00" timezone correction upon datetime transformation
    gdf['storm_start_datetime'] = pd.to_datetime(
        gdf['storm_start_datetime']).dt.tz_convert(None)

    # Phase 1.4: Merge Grid Cells to match towns on grid cell centroid 
    # to town intersection:

    ## Define gdf for ct towns
    ct_towns_gdf = gpd.read_file(ct_towns_shp)

    ## Filter towns to make sure extraneous rows are removed
    ct_towns_gdf = ct_towns_gdf[
        (ct_towns_gdf['STATE'] == 'Connecticut') & 
        (ct_towns_gdf['STATE_COD'] == 'CT') & 
        pd.notna(ct_towns_gdf['MAP_LABEL'])
    ]
    ct_towns_gdf.reset_index(inplace=True)

    ## Further cleaning
    ct_towns_gdf = ct_towns_gdf.drop(columns=['ACREAGE', 'AREA_SQMI', 
    'TOWN_FIELD', 'LABEL_FLAG', 'CT_LEGEND', 'MA_LEGEND', 'ME_LEGEND', 
    'NH_LEGEND', 'NJ_LEGEND', 'NY_LEGEND', 'RI_LEGEND', 'VT_LEGEND', 
    'CT_LABEL_Y', 'CT_LABEL_N', 'LAND_CLASS', "CNTY_FIELD", "CNTY_COD", 
    'STATE_NAME', 'STATE_COD', 'MAP_LABEL', "index"])

    ## Add centroid attribute to grid cells:

    grid_cells_gdf = gpd.read_file(grid_cells_shp)
    grid_cells_gdf['centroid'] = grid_cells_gdf['geometry'].centroid

    ## Specify centroids as active geometry to allow join
    grid_cells_gdf = grid_cells_gdf.set_geometry('centroid')
    
    ## Project both to state plane (CT) for best results
    grid_cells_gdf = grid_cells_gdf.to_crs(epsg=26995)
    ct_towns_gdf = ct_towns_gdf.to_crs(epsg=26995)

    ## Perform spatial join of towns and grid cells
    joined_gc_towns = gpd.sjoin(grid_cells_gdf, ct_towns_gdf, 
                        how="left", predicate="within")

    joined_gc_towns = joined_gc_towns.dropna()

    joined_gc_towns['centroid'] = joined_gc_towns['centroid'].to_crs(epsg=4326)

    # Phase 1.5: Connect grid cell points to weather data:

    '''
    NOTE: The lon and lat columns added to joined_gc_towns 
    in this section are geocodes for GRID CELL CENTROIDS, 
    which are identifiers that are used in the filenames 
    for the grid cell weather data.
    '''

    ## Add lat & long columns

    ## empty lists for lon, lat values
    lon_values = []
    lat_values = []

    ## Iterate over each row in the GeoDataFrame
    for index, row in joined_gc_towns.iterrows():
        ## Access the centroid geometry
        centroid_point = row['centroid']
        
        ## Extract longitude (x) and latitude (y)
        lon_values.append(centroid_point.x)
        lat_values.append(centroid_point.y)

    ## Add the lon/lat as new columns to the gdf

    joined_gc_towns['lon'] = [round(lon, 5) for lon in lon_values]
    joined_gc_towns['lat'] = [round(lat, 3) for lat in lat_values]

    ## Change format for conformity to filenames
    joined_gc_towns['lon'] = joined_gc_towns['lon'].astype(str).str.replace('.', '_')
    joined_gc_towns['lat'] = joined_gc_towns['lat'].astype(str).str.replace('.', '_')

    # Phase 1.6: Build candidate filenames for weather data retrieval: 

    ## Create blank list for candidate filenames
    weather_filenames = []

    for index, row in joined_gc_towns.iterrows():
        # Access latitude and longitude values for the current row
        lat = row['lat']
        lon = row['lon']
        
        ## Create candidate filename using lat/long
        gc_weather_file_pair = f"lat_{lat}_lon_{lon}_time_series_weather.csv"
        
        # Check if the file exists in the target directory
        file_path = os.path.join(weather_data_dir, 
            gc_weather_file_pair) #target repo is weather_data_dir, 
                                  #specified as an argument
        if os.path.exists(file_path):
            # If the file exists, add it to the list
            weather_filenames.append(gc_weather_file_pair)
        else:
            weather_filenames.append('Not found')     

    # Add the filenames to the dataframe for access later on
    joined_gc_towns['filename'] = weather_filenames

#####-----FUNCTION PHASE 2: USE WINDOWING TO MAXIMIZE DRIVER VALUES-----#####

    '''
    Now, the grid cell centroids are all merged to towns, and we can match towns 
    '''

    ## Rename gdf (town outages) for clarity:
    town_outages = gdf

    # Identify and separate events

        ## Note: The gdf contains an entry for each town each time it is
        ## affected by an event. This means that we can collapse the 
        ## entries with the same storm_start_date time to be considered  
        ## one event, and that we can extract which towns are affected 
        ## by that event. Let's do this:

    # Confirm datetime formatting
    town_outages['storm_start_datetime'] = pd.to_datetime(
        town_outages['storm_start_datetime']) 

    event_df = town_outages.groupby('storm_start_datetime')['town'].apply(lambda towns: ', '.join(towns)).reset_index()
    event_df.rename(columns={'town': 'towns_affected'}, inplace=True)

    print(joined_gc_towns)
    print(event_df)

    ### PHASE 2.1: Function for weather data retrieval:

    def weather_data_retrieval(event, weather_data_dir = "/Volumes/JB_Fortress_L3/EEC/merged_csvs"):

        '''
        This function will work within the event df iteration
        process to allow us to succinctly retrieve weather data
        for each event. 

        NOTE: RIGHT NOW, BEFORE HEARING DECISION FROM STERGIOS
        ON DATA PROCESSING TECHNIQUE, THIS FUNCTION SIMPLY
        WILL RETRIEVE DATA FROM ONE GRID CELL, FROM THE 
        ALPHABETICALLY FIRST LISTED TOWN ON THE AFFECTED TOWNS
        LIST FOR AN EVENT. THIS IS ARBITRARY AND WILL BE CHANGED
        LATER ON.

        ARGUMENTS:
        event -> input taken from each row of the events dataframe.
        No need to specify this argument, this will be used later
        implicitly.

        weather_data_dir -> The directory in which the weather
        data (at an hourly resolution) is stored. Defaults to my
        own path, '/Volumes/JB_Fortress_L3/EEC/merged_csvs',
        string input. 
        '''

        # Find the alphabetically-first town listed and extract it
        town = event['towns_affected'].apply(lambda x: x.split(',')[0])

        # Check if we find a match for the town name
        if town in joined_gc_towns['TOWN_NAME'].values:

            # If we do, extract target filename
            filename = joined_gc_towns.loc[joined_gc_towns['TOWN_NAME'] == town, 'filename'].iloc[0]

            # After extracting filename, open file 
                # Build file path
            file_path = weather_data_dir.asstr + '/' + filename.asstr

            weather_data = pd.read_csv(file_path)

            window_maximizer_helper()

        else:
            # If not, report that we cannot
            print("Town '{town}' not found in town_outages")

    def window_maximizer_helper(storm_date_start)

        # PRELIMINARIES:
        ## ID'ing window bounds:

            ## Create day-before threshold
        event_df['24_hours_prior'] = event_df['storm_start_datetime'] - pd.Timedelta(hours=24) 

        ## Create dat-after threshold
        event_df['24_hours_after'] = event_df['storm_start_datetime'] + pd.Timedelta(hours=24) 

        # DEFINING MAXIMUM WIND VELOCITY

        # DEFINING MAXIMUM PRECIPITATION INTENSITY



        return max_wind_velo, max_precip_intensity

    ### PHASE 2.2: TO BE EDITED LATER. EXECUTION OF WEATHER DATA RETRIEVAL ###

    ''' 
        This section will allow us to create the framework for connecting
        the events to weather data. 

        It will start by doing this relatively naively, by only connecting
        to weather data for the one grid cell associated with the first 
        alphabetically listed town in the set of towns affected by the event. 

        Later, this can be adapted to be made more robust by maximizing
        across the group of grid cells associated with all towns listed or 
        through some other means.
    '''

#####-----   FUNCTION PHASE 3: DEFINING OUTPUTS   -----#####

    # driver_maximized_events = 

    # driver_maximized_events.to_csv('../data/driver_maximized_events.csv')
    #return driver_maximized_events

    ## I'll need to come up with a protocol to associate each event with a grid cell (and more importantly, to a merged_csv to retrieve data.) 

```

``` {python}
#| eval: true
result = windowing_maxima()
```


------------------
Diagnostics:
```{python}
#| eval: false
#| echo: false
grid_cells_gdf = gpd.read_file("../data/grid_cell_shapefile/grid_cells.shp")
ct_towns_gdf = gpd.read_file("../data/ct_towns/ct_towns.shp")

ct_towns_gdf = ct_towns_gdf.drop(columns=['ACREAGE', 'AREA_SQMI', 'TOWN_FIELD', 'LABEL_FLAG', 'CT_LEGEND', 'MA_LEGEND', 'ME_LEGEND', 'NH_LEGEND', 'NJ_LEGEND', 'NY_LEGEND', 'RI_LEGEND', 'VT_LEGEND', 'CT_LABEL_Y', 'CT_LABEL_N', 'LAND_CLASS', "CNTY_FIELD", "CNTY_COD", 'STATE_NAME', 'STATE_COD', 'MAP_LABEL'])

# Reproject both shapefiles to the Connecticut State Plane coordinate system (EPSG:26995)
grid_cells_gdf = grid_cells_gdf.to_crs(epsg=26995)
ct_towns_gdf = ct_towns_gdf.to_crs(epsg=26995)


# Calculate centroids for grid cells
grid_cells_gdf['centroid'] = grid_cells_gdf['geometry'].centroid

# Set centroid as active geometry for the grid cells
grid_cells_gdf = grid_cells_gdf.set_geometry('centroid')

# Plotting the map
fig, ax = plt.subplots(figsize=(10, 10))

# Plot towns (polygons)
ct_towns_gdf.plot(ax=ax, color='lightblue', edgecolor='black', alpha=0.5)

# Plot grid cell centroids (points)
grid_cells_gdf.plot(ax=ax, color='red', markersize=5, label='Centroids')

# Add title and legend
ax.set_title('Towns and Grid Cell Centroids', fontsize=16)
ax.legend()

# Show the plot
plt.show()

joined_gc_towns = gpd.sjoin(grid_cells_gdf, ct_towns_gdf, how="left", predicate="within")

joined_gc_towns = joined_gc_towns.dropna()

fig, ax = plt.subplots(figsize=(10, 10))

# Plot the towns as polygons (light blue)
ct_towns_gdf.plot(ax=ax, color='lightblue', edgecolor='black', alpha=0.5)

# Plot the centroids from the 'centroid' column (red points)
joined_gc_towns.plot(ax=ax, color='red', markersize=5, label='Grid Cell Centroids')

# Add title and legend
ax.set_title('Towns and Grid Cell Centroids', fontsize=16)
ax.legend()

# Show the plot
plt.show()
```