---
author: Jack Bienvenue
title: Windowing for Driver Maximization
format: html
date: 11 March 2025
---

# Introduction 

When constructing the outage prediction model, we make the assumption that outages are caused primarily by high intensity drivers causing disruptions which lead to outages. 

Outage events are sorted into individual days. For storm events, the distinction of days is arbitrary. This file introduces a method for windowing a 24-hour period before and after an outage event is recorded (for a 48 hour window) and identifying the maximum mean daily values for drivers within the 24 fabricated "pseudo-days" which can be formed by incrementing 24 hour periods hourly within the 48 hour window.

``` {python}
# Package Import
import pandas as pd
import geopandas as gpd
import folium
import matplotlib.pyplot as plt
import os
import warnings

# After validating that these messsages do not apply for this script, suppress:
warnings.filterwarnings("ignore", 
    message="Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect.")
```

Begin with the shapefile of municipalities for Connecticut. (Here, it is fabricated using a shapefile of CT and the set of Northeast towns)

# Data Preparation

``` {python}
#| echo: true

# Step 1: Load northeast towns, ct boundary shapefile
towns_gdf = gpd.read_file('../data/northeast_towns_shapefile/Northeast_Town_Polygon.shp')
ct_gdf = gpd.read_file('../data/ct_shapefile/Connecticut_Poly.shp')

# Step 2: Put shapefiles in the same CRS (if they are not already)
towns_gdf = towns_gdf.to_crs(ct_gdf.crs)

# Step 3: Perform the intersection, getting CT towns
ct_towns_gdf = gpd.overlay(towns_gdf, ct_gdf, how='intersection')

# Step 4: save result
ct_towns_gdf.to_file('../data/ct_towns/ct_towns.shp')

# Plot to confirm
ct_towns_gdf.plot()
```

# Function Construction

``` {python}
#| eval: true

# Data Import

def windowing_maxima(town_shapefile = "../data/ct_towns/ct_towns.shp", 
    outage_data = "../data/town_outages.csv", 
    weather_data_dir = "/Volumes/JB_Fortress_L3/EEC/merged_csvs", 
    grid_cell_shapefile = "../data/grid_cell_shapefile/grid_cells.shp"):

    '''
    This function...

    ARGUMENTS:

    town_shapefile - the argument takes the town shapefile as an input 
    as a spatial merging field

    outage_data - the csv file holding information about outages events 
    by town

        Fields for outage_data file:
            - town ("Town")
            - storm_start_datetime ('YYYY-MM-DD HH:MM:SS+00:00')
            - outages (count data)
            - customers_affected (count data)

    weather_data_dir - The directory which is home to HOURLY csv time 
    series of weather data

    grid_cell_shapefile - the relative path to the shapefile containing 
    the grid cells for the state of interest

    '''

#####-----   FUNCTION PHASE 1: READ IN DATA, MERGE GRID CELLS TO TOWNS    -----#####

    # Phase 1.1: READ IN SHAPEFILE FOR CT TOWNS, GRID CELLS

    ct_towns_shp = town_shapefile # from arg
    grid_cells_shp = grid_cell_shapefile # from arg

    # Phase 1.2: READ IN OUTAGE DATA

    gdf = gpd.read_file(outage_data)

    # Phase 1.3: Define windows for events:

        ### FOR SPEED OF DEVELOPMENT:

    gdf = gdf.head()

        ### -------DELETE LATER---------

    ## Remove extraneous "+00:00" timezone correction upon datetime transformation
    gdf['storm_start_datetime'] = pd.to_datetime(
        gdf['storm_start_datetime']).dt.tz_convert(None)

    # Phase 1.4: Merge Grid Cells to match towns on grid cell centroid 
    # to town intersection:

    ## Define gdf for ct towns
    ct_towns_gdf = gpd.read_file(ct_towns_shp)

    ## Filter towns to make sure extraneous rows are removed
    ct_towns_gdf = ct_towns_gdf[
        (ct_towns_gdf['STATE'] == 'Connecticut') & 
        (ct_towns_gdf['STATE_COD'] == 'CT') & 
        pd.notna(ct_towns_gdf['MAP_LABEL'])
    ]
    ct_towns_gdf.reset_index(inplace=True)

    ## Further cleaning
    ct_towns_gdf = ct_towns_gdf.drop(columns=['ACREAGE', 'AREA_SQMI', 
    'TOWN_FIELD', 'LABEL_FLAG', 'CT_LEGEND', 'MA_LEGEND', 'ME_LEGEND', 
    'NH_LEGEND', 'NJ_LEGEND', 'NY_LEGEND', 'RI_LEGEND', 'VT_LEGEND', 
    'CT_LABEL_Y', 'CT_LABEL_N', 'LAND_CLASS', "CNTY_FIELD", "CNTY_COD", 
    'STATE_NAME', 'STATE_COD', 'MAP_LABEL', "index"])

    ## Add centroid attribute to grid cells:

    grid_cells_gdf = gpd.read_file(grid_cells_shp)
    grid_cells_gdf['centroid'] = grid_cells_gdf['geometry'].centroid

    ## Specify centroids as active geometry to allow join
    grid_cells_gdf = grid_cells_gdf.set_geometry('centroid')
    
    ## Project both to state plane (CT) for best results
    grid_cells_gdf = grid_cells_gdf.to_crs(epsg=26995)
    ct_towns_gdf = ct_towns_gdf.to_crs(epsg=26995)

    ## Perform spatial join of towns and grid cells
    joined_gc_towns = gpd.sjoin(grid_cells_gdf, ct_towns_gdf, 
                        how="left", predicate="within")

    joined_gc_towns = joined_gc_towns.dropna()

    joined_gc_towns['centroid'] = joined_gc_towns['centroid'].to_crs(epsg=4326)

    # Phase 1.5: Connect grid cell points to weather data:

    '''
    NOTE: The lon and lat columns added to joined_gc_towns 
    in this section are geocodes for GRID CELL CENTROIDS, 
    which are identifiers that are used in the filenames 
    for the grid cell weather data.
    '''

    ## Add lat & long columns

    ## empty lists for lon, lat values
    lon_values = []
    lat_values = []

    ## Iterate over each row in the GeoDataFrame
    for index, row in joined_gc_towns.iterrows():
        ## Access the centroid geometry
        centroid_point = row['centroid']
        
        ## Extract longitude (x) and latitude (y)
        lon_values.append(centroid_point.x)
        lat_values.append(centroid_point.y)

    ## Add the lon/lat as new columns to the gdf

    joined_gc_towns['lon'] = [round(lon, 5) for lon in lon_values]
    joined_gc_towns['lat'] = [round(lat, 3) for lat in lat_values]

    ## Change format for conformity to filenames
    joined_gc_towns['lon'] = joined_gc_towns['lon'].astype(str).str.replace('.', '_')
    joined_gc_towns['lat'] = joined_gc_towns['lat'].astype(str).str.replace('.', '_')

    # Phase 1.6: Build candidate filenames for weather data retrieval: 

    ## Create blank list for candidate filenames
    weather_filenames = []

    for index, row in joined_gc_towns.iterrows():
        # Access latitude and longitude values for the current row
        lat = row['lat']
        lon = row['lon']
        
        ## Create candidate filename using lat/long
        gc_weather_file_pair = f"lat_{lat}_lon_{lon}_time_series_weather.csv"
        
        # Check if the file exists in the target directory
        file_path = os.path.join(weather_data_dir, 
            gc_weather_file_pair) #target repo is weather_data_dir, 
                                  #specified as an argument
        if os.path.exists(file_path):
            # If the file exists, add it to the list
            weather_filenames.append(gc_weather_file_pair)
        else:
            weather_filenames.append('Not found')     

    # Add the filenames to the dataframe for access later on
    joined_gc_towns['filename'] = weather_filenames

#####-----FUNCTION PHASE 2: USE WINDOWING TO MAXIMIZE DRIVER VALUES-----#####

    '''
    Now, the grid cell centroids are all merged to towns, and we can match towns 
    '''

    ## Rename gdf (town outages) for clarity:
    town_outages = gdf

    # Identify and separate events

        ## Note: The gdf contains an entry for each town each time it is
        ## affected by an event. This means that we can collapse the 
        ## entries with the same storm_start_date time to be considered  
        ## one event, and that we can extract which towns are affected 
        ## by that event. Let's do this:

    # Confirm datetime formatting
    town_outages['storm_start_datetime'] = pd.to_datetime(
        town_outages['storm_start_datetime']) 

    # Confirm numeric types for to-be-summed vars:
    town_outages['outages'] = pd.to_numeric(town_outages['outages'], errors='coerce')
    town_outages['customers_affected'] = pd.to_numeric(town_outages['customers_affected'], errors='coerce')


    # Form event_df by grouping storms by the same start time
    event_df = town_outages.groupby('storm_start_datetime').agg(
        towns_affected=('town', lambda towns: ', '.join(towns)),   # Concatenate towns
        outages=('outages', 'sum'),                       # Sum outages
        customers_affected=('customers_affected', 'sum')  # Sum customers affected
        ).reset_index()

    print('joined gc towns:', joined_gc_towns, "\n\n\n")
    print('unique towns:', joined_gc_towns['TOWN_NAME'].value_counts())
    print('town outages:', town_outages, '\n\n\n')
    #####print('Event df early definiton:', event_df, "\n\n\n")

    ### PHASE 2.1: Function for weather data retrieval:

    def weather_data_retrieval(event, weather_data_dir = "/Volumes/JB_Fortress_L3/EEC/merged_csvs"):

        '''
        This function will work within the event df iteration
        process to allow us to succinctly retrieve weather data
        for each event. 

        NOTE: RIGHT NOW, BEFORE HEARING DECISION FROM STERGIOS
        ON DATA PROCESSING TECHNIQUE, THIS FUNCTION SIMPLY
        WILL RETRIEVE DATA FROM ONE GRID CELL, FROM THE 
        ALPHABETICALLY FIRST LISTED TOWN ON THE AFFECTED TOWNS
        LIST FOR AN EVENT. THIS IS ARBITRARY AND WILL BE CHANGED
        LATER ON.

        ARGUMENTS:
        event -> input taken from each row of the events dataframe.
        No need to specify this argument, this will be used later
        implicitly.

        weather_data_dir -> The directory in which the weather
        data (at an hourly resolution) is stored. Defaults to my
        own path, '/Volumes/JB_Fortress_L3/EEC/merged_csvs',
        string input. 
        '''

        # Find the alphabetically-first town listed and extract it
        town = event['towns_affected'].split(',')[0]

        # Check if we find a match for the town name
        if town in joined_gc_towns['TOWN_NAME'].values:

            # If we do, extract target filename
            filename = joined_gc_towns.loc[joined_gc_towns['TOWN_NAME'] == town, 'filename'].iloc[0]

            # After extracting filename, open file 
                # Build file path
            file_path = weather_data_dir + '/' + filename

            # Read in relevant weather data
            weather_data = pd.read_csv(file_path)

            # Use the helper function to get maximum values
            max_wind_velo, max_precip_intensity = window_maximizer_helper(weather_data=weather_data, storm_start_datetime=event['storm_start_datetime'])

            event['max_wind_velo'] = max_wind_velo
            event['max_precip_intensity'] = max_precip_intensity

        else:
            # If not, report that we cannot find the town
            print("Town '{town}' not found in town_outages")
            event['max_wind_velo'] = None
            event['max_precip_intensity'] = None

        return event

    def window_maximizer_helper(weather_data, storm_start_datetime):

        # PRELIMINARIES:

        ## ID'ing window bounds:
        ### Create day-before threshold
        prior_25 = event_df['storm_start_datetime'] - pd.Timedelta(hours=25) 
        '''
        Why 25 hours here instead of 24? Precipiation is measured cumulatively
        for each day in our dataset, so we need to extract an extra entry before
        our first windowed hour (hour = -24:00:00 from storm start) so that we
        can subtract the already-accumulated precipitation and convert the rest of
        entries to allow list the hourly intensity.
        '''
        ### Create day-after threshold
        after_24 = event_df['storm_start_datetime'] + pd.Timedelta(hours=24) 

        ## Subset weather data to prior_24 -1 hour, after_24.

        ### Ensure datetime format to begin
        weather_data['time'] = pd.to_datetime(weather_data['time'])
        event_df['storm_start_datetime'] = pd.to_datetime(event_df['storm_start_datetime'])

        ### Perform subsetting
        weather_event_subset = weather_data[(weather_data['time'] >= prior_25.iloc[0]) & 
                                    (weather_data['time'] <= after_24.iloc[0])]

        #####print(weather_event_subset)

        # DEFINING MAXIMUM WIND VELOCITY

        ## Triangulate wind vectors to get wind velocity
        weather_data['wind_velo'] = (weather_data['u10']**2 + weather_data['v10']**2)**(0.5)

        ## Drop first (-25th) hour so it does not get selected
        weather_data_without_first = weather_data.drop(index=0)

        ## Identify maximum
        max_wind_velo = round(weather_data_without_first['wind_velo'].max(), 5)

        # DEFINING MAXIMUM PRECIPITATION INTENSITY

            ## Special attention to cumulative nature of entries

        ## Calculate hourly rate by subtracting previous entry
        weather_data['precip_intensity'] = weather_data['tp'].diff().fillna(0)
            ### Note, this fills NA values with 0, which is intended to ensure 
            ### that artificially high values don't get included and selected.

        ## Indicate when day changes occur
        weather_data['day_change'] = (weather_data['time'].dt.hour == 1) & weather_data['time'].dt.date.diff().ne(pd.Timedelta(0))
            ### This is very important, as weather events start at different
            ### times. We need to know how to identify the day changes for 
            ### calculation based on cumulative values for ['tp'].
            ### This finds when its a new day & 1am, where cumulative values
            ### effectively reset in our dataframe.

        ## Reset values for new day
        weather_data['precip_intensity'] = weather_data.apply(
            lambda row: row['tp'] if row['day_change'] else row['precip_intensity'], axis=1)

        ## Remove day_change indicator column 
        weather_data.drop('day_change', axis=1, inplace=True)

        ## Drop first (-25th) hour so it does not get selected
        weather_data_without_first = weather_data.drop(index=0)

        ## Define maximum precipitation intensity for the window
        max_precip_intensity = round(weather_data_without_first['precip_intensity'].max(), 5)
            ### We're using weather_data_without_first here because we no longer
            ### need to consider the extra value which was just there for proper
            ### calculation from cumulative values for precipitation.

        ## Return the values
        return max_wind_velo, max_precip_intensity

#####-----   FUNCTION PHASE 3: DEFINING OUTPUTS   -----#####

    ''' 
        This section will allow us to execute the framework for connecting
        the events to weather data. 

        It will start by doing this relatively naively, by only connecting
        to weather data for the one grid cell associated with the first 
        alphabetically listed town in the set of towns affected by the event. 

        Later, this can be adapted to be made more robust by maximizing
        across the group of grid cells associated with all towns listed or 
        through some other means.
    '''

    # PHASE 3.1 - Defining maximum data through use of functions created previously

    # Iterate through event entries, maximizing drivers and adding them to df

    ## Define empty list to collect driver-maximized events
    driver_maximized_events_list = []

    ## Iterate through event entries, maximizing drivers and adding them to list
    for index, row in event_df.iterrows():  # iterrows returns (index, row) tuple
        maximized_driver_event = weather_data_retrieval(row)
        driver_maximized_events_list.append(maximized_driver_event)

    ## After loop, convert list into DataFrame
    driver_maximized_events = pd.DataFrame(driver_maximized_events_list)

    # PHASE 3.2 - Merge back to event_df 

    '''
    Now that we have extracted maximum values, let's merge our new 
    driver covariates back into our events df to have outcomes
    and covariates in the same place.
    '''

    #####print(event_df, "\n\n\n\n\n")
    #####print(driver_maximized_events, "\n\n\n\n\n")

    final_df = pd.merge(event_df, 
                     driver_maximized_events[['storm_start_datetime', 'towns_affected', 'max_wind_velo', 'max_precip_intensity']], 
                     on=['storm_start_datetime', 'towns_affected'], 
                     how='left')

    #####print(final_df)

    # PHASE 3.3 - Return & save data

    # Save out to file
    driver_maximized_events.to_csv('../data/driver_maximized_events.csv')
    
    # Also return the df locally to evaluate
    return driver_maximized_events
```

``` {python}
#| eval: true
result = windowing_maxima()
print(result)
```

------------------
Diagnostics:
```{python}
#| eval: false
#| echo: false
grid_cells_gdf = gpd.read_file("../data/grid_cell_shapefile/grid_cells.shp")
ct_towns_gdf = gpd.read_file("../data/ct_towns/ct_towns.shp")

ct_towns_gdf = ct_towns_gdf.drop(columns=['ACREAGE', 'AREA_SQMI', 'TOWN_FIELD', 'LABEL_FLAG', 'CT_LEGEND', 'MA_LEGEND', 'ME_LEGEND', 'NH_LEGEND', 'NJ_LEGEND', 'NY_LEGEND', 'RI_LEGEND', 'VT_LEGEND', 'CT_LABEL_Y', 'CT_LABEL_N', 'LAND_CLASS', "CNTY_FIELD", "CNTY_COD", 'STATE_NAME', 'STATE_COD', 'MAP_LABEL'])

# Reproject both shapefiles to the Connecticut State Plane coordinate system (EPSG:26995)
grid_cells_gdf = grid_cells_gdf.to_crs(epsg=26995)
ct_towns_gdf = ct_towns_gdf.to_crs(epsg=26995)


# Calculate centroids for grid cells
grid_cells_gdf['centroid'] = grid_cells_gdf['geometry'].centroid

# Set centroid as active geometry for the grid cells
grid_cells_gdf = grid_cells_gdf.set_geometry('centroid')

# Plotting the map
fig, ax = plt.subplots(figsize=(10, 10))

# Plot towns (polygons)
ct_towns_gdf.plot(ax=ax, color='lightblue', edgecolor='black', alpha=0.5)

# Plot grid cell centroids (points)
grid_cells_gdf.plot(ax=ax, color='red', markersize=5, label='Centroids')

# Add title and legend
ax.set_title('Towns and Grid Cell Centroids', fontsize=16)
ax.legend()

# Show the plot
plt.show()

joined_gc_towns = gpd.sjoin(grid_cells_gdf, ct_towns_gdf, how="left", predicate="within")

joined_gc_towns = joined_gc_towns.dropna()

fig, ax = plt.subplots(figsize=(10, 10))

# Plot the towns as polygons (light blue)
ct_towns_gdf.plot(ax=ax, color='lightblue', edgecolor='black', alpha=0.5)

# Plot the centroids from the 'centroid' column (red points)
joined_gc_towns.plot(ax=ax, color='red', markersize=5, label='Grid Cell Centroids')

# Add title and legend
ax.set_title('Towns and Grid Cell Centroids', fontsize=16)
ax.legend()

# Show the plot
plt.show()
```