---
title: Aggregating Hourly Driver Data over Daily Increments
format: html
author: Jack Bienvenue
Date: "February 14, 2025"
---

Now, we have conquered the tricky parts of importing grib files and processing them into individual CSVs, including at an hourly resolution.

Let's continue onward by taking advantage of the newly cleaned data by aggregating relevant variables over individual days.

We will seek the following information:

- Total precipitation over day
- Average wind speed over day
- High temperature
- Low temperature

``` {python}

# Package import
import pandas as pd
import os

''' 
Let's define a function to perform the aggregation:

ARGUMENTS:

input_directory - input directory which houses the target files (enter as relative path)

output_path - output path for the NEW directory that 
    is meant to store the new, aggregated CSVs (enter as path)

NOTE: THIS FUNCTION WILL ONLY RETURN USEFUL AGGREGATIONS WHEN GIVEN DATA WITH HOURLY GRANULARITY AS AN INPUT.
'''

def daily_aggregator(input_directory, output_path):

    # Initialize empty list to store DataFrames
    df_list = []

    # Iterate through all files in the input directory:
    files = os.listdir(input_directory)

    # Pick out the CSV files exclusively
    csv_files = [file for file in files if file.endswith('.csv')]

    for csv in csv_files:
        # Pick out the paths to each of the CSVs
        csv_path = os.path.join(input_directory, csv)

        # Read in CSV as dataframe
        df = pd.read_csv(csv_path)
        df_list.append(df)

    for df in df_list:
        # Read in 'time' column as datetime to begin aggregating
        df['time'] = pd.to_datetime(df['time'], format='%Y-%m-%d %H:%M:%S')

        # Access date by extracting it 
        df['date'] = df['time'].dt.date

        # Pre-processing for aggregation (wind speed triangulation)

        df['wind_speed'] = ((df['u10'])**2 + (df['v10'])**2)**(0.5)

        # Aggregation: 

        daily_aggregated_df = df.groupby('date').agg(
            # Sum 'tp' to get total precipitation
            total_precipitation=('tp', 'sum'),
            # Find high temp
            high_temperature=('t2m', 'max'),
            # Find low temp
            low_temperature=('t2m', 'min'),
            # 
            avg_wind_speed=('wind_speed', 'max')
            ).reset_index()

#-----------------------------------------------------------
# EXPORT FILES

    def create_directory(output_path):
        if not os.path.exists(output_path):
            os.makedirs(output_path)

    def export_dfs_to_csv(dfs, output_path): 
        create_directory(output_path) # Ensure directory exists

            # Iterate through the list of DataFrames
        for idx, df in enumerate(dfs):

            lat = df['latitude'].iloc[0]
            lon = df['longitude'].iloc[0]
            lon = lon.round(5) # for consistency and brevity

            # Create a filename using latitude, longitude, and an index
            filename = f"lat_{lat}_lon_{lon}_time_series_weather.csv"

            # Replace dots in latitude and longitude with underscores
            filename = filename.rsplit('.', 1)  # Split only on the last dot (the file extension)
            filename[0] = filename[0].replace('.', '_')  # Replace dots in the main part of the filename

            # Join the filename back together
            filename = '.'.join(filename)
            
            # Construct the full path for the CSV file
            full_path = os.path.join(output_path, filename)

            # Compensate for error:
            df = df.sort_values(by='time')

            # Export the DataFrame to CSV
            df.to_csv(full_path, index=False)

    export_dfs_to_csv(df_list, output_path)

```

Let's try the function:

``` {python}
daily_aggregator('../data/fix_hourly_replicate_issue/', '../data/aggregation_trial/')
```